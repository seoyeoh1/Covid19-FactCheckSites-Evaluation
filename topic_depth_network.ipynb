{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating co-occurence network for visualization with Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "naver_df = pd.read_csv('/Users/SeoyeonHong/Desktop/text_mining/topic_modeling/naver_kin/naver_q1_topic_representation.tsv', sep = '\\t')\n",
    "aha_df = pd.read_csv('/Users/SeoyeonHong/Desktop/text_mining/topic_modeling/aha/aha_qna_topic_representation.tsv', sep = '\\t')\n",
    "hidoc_df = pd.read_csv('/Users/SeoyeonHong/Desktop/text_mining/topic_modeling/hidoc/hidoc_qna_topic_representation.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic1_naver = naver_df.loc[naver_df['Topic_Num'] == 2]\n",
    "topic1_aha_1 = aha_df.loc[aha_df['Topic_Num'] == 23]\n",
    "#topic1_aha_2 = aha_df.loc[aha_df['Topic_Num'] == 26]\n",
    "#topic1_aha_3 = aha_df.loc[aha_df['Topic_Num'] == 18]\n",
    "#topic1_aha = pd.concat([topic1_aha_1, topic1_aha_2])\n",
    "#topic1_hidoc_1 = hidoc_df.loc[hidoc_df['Topic_Num'] == 9]\n",
    "#topic1_hidoc_2 = hidoc_df.loc[hidoc_df['Topic_Num'] == 15]\n",
    "#topic1_hidoc = pd.concat([topic1_hidoc_1, topic1_hidoc_2])\n",
    "\n",
    "#extra = [\"안녕하세요 의사선생님 궁금한게 있어서 질문드립니다 할머니가 지금 폐렴에 걸리셨습니다 근데 기침도 안하시고 하는데 폐렴이랍니다 ㅠㅠ 근데 어머니가 70세이상은 보호자가 꼭 필요하다면서 오늘 할머니 옆에서 주무시기로 했습니다 1인실 특실입니다 창문도 입고요 ㅠㅠ 저는 걱정되는게 폐렴이 전염성이 있어 롬길까봐 걱정입니다 창문도 있고 다 있지만요 폐렴 전염성이 높은건가요? 할머니가 이상하게 기침은 안하신다는데 그리고 할머니는 마스크를 착용 하지 않고 있습니다 어머니는 KF94 마스크 착용하고 계시구요 만약에 오늘 어머니가 그 병실에 자면은 폐렴에 전염 되지 않나요? 어머니는 K94 마스크를 낀 채로 주무신다고 합니다 더 걱정되는건 1인실 창문은 있지만 밀폐된 공간이라 걱정이 많이 되는네요 ㅠㅠ 지금 이 시국에 너무 걱정입니다 폐렴 전염성이 높을까요? 2일뒤면 저도 가야되는데 걱정되어 죽겠습니다 좀 도와주세요\", \"아파트 인터폰 현관앞에 사람들 많이 지나가는 길에 핸드폰이랑 이어폰을 떨어뜨렸고 그걸 맨손으로 주웠습니다.근데 땅바닥은 사람들이 신발로 밟고 지나가잖아요그럼 침이나 가래같은 걸 밟고 땅에도 묻어있을텐데 거기에 핸드폰을 떨어뜨렸으니 바이러스가 혹시라도 묻어서 그걸 집거나 공기중으로 전파되어 제가 감염될수 있을지 너무 불안합니다..확진자인진 모르지만 사람들이 여러군데 다니고 하잖아요 그럼 그중에 확진자나 잠복기에 있는 사람의 가래나 침이 땅바닥에 묻어있거나 그걸 밟은 신발로 다니니 땅바닥에도 우한폐렴 바이러스가 있을거 같은데요지금은 물티슈로 닦았는데 안심해도 되나요 아님 우한폐렴 걸릴수도 있나요?근데 이미 닦았다고 해도 공기중으로 감염되면 어떡하죠?\", \"아는사람이 신종플루라는데 타미플루를 처방받고 5일이 지났는데도 잔증상(기침같은)이 남아 계속 병원을 다닙니다2주째 계속되고 있는데 본인은 많이 나아졌다고 하는데 저는 불안합니다이사람이랑 접촉해도 안 옮을까요?\", \"안녕하세요~요즘 코로나 바이러스 때문에 걱정이 많은데요 카페에서 일을하니 주문을 받기 때문에 여러사람과 대화를 하는데 중국사람들 당골도 많습니다 오늘은 평소에 자주오는 중국 여자 손님과 가까이서 얘기를 했는데 손님은 마스크를 쓰지 않았고 저는 쓰고 몇분얘기했는데 손님이 가고난후에 아차하는 생각과 걱정이 되네요 중국인이라는 생각에 더걱정이 되네요 이런일로 걱정을 해야하는 현실이 너무 싫고 요즘 너무 우울합니다 그리고 사람들과 주문을 받는 과정에서 대화를 해야하는데 어떻게 해야할지요\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7 entries, 224 to 230\n",
      "Data columns (total 5 columns):\n",
      "Unnamed: 0            7 non-null int64\n",
      "Topic_Num             7 non-null float64\n",
      "Topic_Perc_Contrib    7 non-null float64\n",
      "Keywords              7 non-null object\n",
      "Text                  7 non-null object\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 336.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "topic1_aha_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "mecab = Mecab()\n",
    "mecab_nouns = []\n",
    "    \n",
    "for sent in topic1_aha['Text']: #21\n",
    "    substring = re.sub(r'[^\\w\\s]','',str(sent))\n",
    "    substring = ''.join([i for i in str(substring) if not i.isdigit()])\n",
    "    substring = str(substring).replace(\"가능\",\"\").replace(\"정도\",\"\").replace(\"관련\",\"\").replace(\"지금\",\"\").replace(\"월일\",\"\").replace(\"가요\",\"\").replace(\"동안\",\"\").replace(\"요즘\",\"\").replace(\"평소\",\"\").replace(\"최근\",\"\").replace(\"느낌\",\"\").replace(\"하루\",\"\").replace(\"시간\",\"\").replace(\"오늘\",\"\").replace(\"동안\",\"\").replace(\"새벽\",\"\").replace(\"그때\",\"\").replace(\"예전\",\"\").replace(\"코로나\", \"\").replace(\"면\", \"\").replace(\"도\", \"\").replace(\"은\", \"\").replace(\"임\", \"\").replace(\"글\", \"\").replace(\"감사\", \"\").replace(\"시\", \"\").replace(\"때\", \"\").replace(\"곳\", \"\").replace(\"문\", \"\").replace(\"말\", \"\").replace(\"코로나바이러스\", \"\")\n",
    "    sent_pos = mecab.pos(substring)\n",
    "    nouns = [n for n, tag in sent_pos if tag in [\"NNG\",\"NNP\"] ]\n",
    "    #print(nouns)\n",
    "    mecab_nouns.append(nouns)\n",
    "\n",
    "\n",
    "for sent in topic1_hidoc['Text']: #\n",
    "    substring = re.sub(r'[^\\w\\s]','',str(sent))\n",
    "    substring = ''.join([i for i in str(substring) if not i.isdigit()])\n",
    "    substring = str(substring).replace(\"가능\",\"\").replace(\"정도\",\"\").replace(\"관련\",\"\").replace(\"지금\",\"\").replace(\"월일\",\"\").replace(\"가요\",\"\").replace(\"동안\",\"\").replace(\"요즘\",\"\").replace(\"평소\",\"\").replace(\"최근\",\"\").replace(\"느낌\",\"\").replace(\"하루\",\"\").replace(\"시간\",\"\").replace(\"오늘\",\"\").replace(\"동안\",\"\").replace(\"새벽\",\"\").replace(\"그때\",\"\").replace(\"예전\",\"\").replace(\"전\",\"\").replace(\"후\",\"\").replace(\"닦\",\"\").replace(\"답변\",\"\").replace(\"안녕\",\"\").replace(\"제목\",\"\").replace(\"도\",\"\").replace(\"나용\",\"\").replace(\"번\",\"\").replace(\"애요\",\"\").replace(\"쌀\",\"\").replace(\"정\",\"\").replace(\"질문\",\"\").replace(\"고\",\"\").replace(\"때\",\"\").replace(\"첨\",\"\").replace(\"칸\",\"\").replace(\"소간\",\"\").replace(\"일\",\"\").replace(\"의\",\"\").replace(\"상\",\"\").replace(\"일\",\"\").replace(\"코로나\",\"\").replace(\"대요\",\"\").replace(\"자\",\"\").replace(\"글\",\"\").replace(\"시\", \"\").replace(\"코로나바이러스\",\"\").replace(\"문\",\"\").replace(\"달\",\"\")\n",
    "    sent_pos = mecab.pos(substring)\n",
    "    nouns = [n for n, tag in sent_pos if tag in [\"NNG\",\"NNP\"] ]\n",
    "    #print(nouns)\n",
    "    mecab_nouns.append(nouns)\n",
    "    \n",
    "for sent in topic1_naver['Text']: #1701\n",
    "    substring = re.sub(r'[^\\w\\s]','',str(sent))\n",
    "    substring = ''.join([i for i in str(substring) if not i.isdigit()])\n",
    "    substring = str(substring).replace(\"가능\",\"\").replace(\"정도\",\"\").replace(\"관련\",\"\").replace(\"지금\",\"\").replace(\"월일\",\"\").replace(\"가요\",\"\").replace(\"동안\",\"\").replace(\"요즘\",\"\").replace(\"평소\",\"\").replace(\"최근\",\"\").replace(\"느낌\",\"\").replace(\"하루\",\"\").replace(\"시간\",\"\").replace(\"오늘\",\"\").replace(\"동안\",\"\").replace(\"새벽\",\"\").replace(\"그때\",\"\").replace(\"예전\",\"\").replace(\"▲\",\"\").replace(\"◇\",\"\").replace(\"-\",\"\").replace(\"코로나\", \"\").replace(\"코로나바이러스\",\"\").replace(\"내공\",\"\").replace(\"질문\",\"\").replace(\"답변\",\"\").replace(\"안녕하세요\",\"\")\n",
    "    sent_pos = mecab.pos(substring)\n",
    "    nouns = [n for n, tag in sent_pos if tag in [\"NNG\",\"NNP\"] ]\n",
    "    #print(nouns)\n",
    "    mecab_nouns.append(nouns)\n",
    "\n",
    "#for sent in extra:\n",
    "#    substring = re.sub(r'[^\\w\\s]','',str(sent))\n",
    "#    substring = ''.join([i for i in str(substring) if not i.isdigit()])\n",
    "#    substring = str(substring).replace(\"전\",\"\").replace(\"후\",\"\").replace(\"닦\",\"\").replace(\"답변\",\"\").replace(\"안녕\",\"\").replace(\"제목\",\"\").replace(\"도\",\"\").replace(\"나용\",\"\").replace(\"번\",\"\").replace(\"애요\",\"\").replace(\"쌀\",\"\").replace(\"정\",\"\").replace(\"질문\",\"\").replace(\"고\",\"\").replace(\"때\",\"\").replace(\"첨\",\"\").replace(\"칸\",\"\").replace(\"소간\",\"\").replace(\"일\",\"\").replace(\"의\",\"\").replace(\"상\",\"\").replace(\"일\",\"\").replace(\"코로나\",\"\").replace(\"대요\",\"\").replace(\"자\",\"\").replace(\"글\",\"\").replace(\"시\", \"\").replace(\"코로나바이러스\",\"\").replace(\"문\",\"\").replace(\"달\",\"\")\n",
    "#    sent_pos = mecab.pos(substring)\n",
    "#    nouns = [n for n, tag in sent_pos if tag in [\"NNG\",\"NNP\"] ]\n",
    "#    #print(nouns)\n",
    "#    mecab_nouns.append(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5150"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mecab_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "class Cooccurrence(CountVectorizer):\n",
    "    \"\"\"Co-ocurrence matrix\n",
    "    Convert collection of raw documents to word-word co-ocurrence matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : string, 'utf-8' by default.\n",
    "        If bytes or files are given to analyze, this encoding is used to\n",
    "        decode.\n",
    "\n",
    "    ngram_range : tuple (min_n, max_n)\n",
    "        The lower and upper boundary of the range of n-values for different\n",
    "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "        will be used.\n",
    "\n",
    "    max_df: float in range [0, 1] or int, default=1.0\n",
    "\n",
    "    min_df: float in range [0, 1] or int, default=1\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "\n",
    "    >> import Cooccurrence\n",
    "    >> docs = ['this book is good',\n",
    "               'this cat is good',\n",
    "               'cat is good shit']\n",
    "    >> model = Cooccurrence()\n",
    "    >> Xc = model.fit_transform(docs)\n",
    "\n",
    "    Check vocabulary by printing\n",
    "    >> model.vocabulary_\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='utf-8', ngram_range=(1, 1),\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 stop_words=None, normalize=True, vocabulary=None):\n",
    "\n",
    "        super(Cooccurrence, self).__init__(\n",
    "            ngram_range=ngram_range,\n",
    "            max_df=max_df,\n",
    "            min_df=min_df,\n",
    "            max_features=max_features,\n",
    "            stop_words=stop_words,\n",
    "            vocabulary=vocabulary\n",
    "        )\n",
    "\n",
    "        self.X = None\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Fit cooccurrence matrix\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Xc : Cooccurrence matrix\n",
    "\n",
    "        \"\"\"\n",
    "        X = super(Cooccurrence, self).fit_transform(raw_documents)\n",
    "        self.X = X\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        Xc = (X.T * X)\n",
    "        if self.normalize:\n",
    "            g = sp.diags(1./Xc.diagonal())\n",
    "            Xc = g * Xc\n",
    "        else:\n",
    "            Xc.setdiag(0)\n",
    "\n",
    "        return Xc\n",
    "\n",
    "    def vocab(self):\n",
    "        tuples = super(Cooccurrence, self).get_feature_names()\n",
    "        vocabulary=[]\n",
    "        for e_tuple in tuples:\n",
    "            tokens = e_tuple.split()\n",
    "            for t in tokens:\n",
    "                if t not in vocabulary:\n",
    "                    vocabulary.append(t)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    def word_histgram(self):\n",
    "        word_list = super(Cooccurrence, self).get_feature_names()\n",
    "        count_list = self.X.toarray().sum(axis=0)\n",
    "        return dict(zip(word_list,count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "class BaseCooccurrence:\n",
    "    INPUT=[list,str]\n",
    "    OUTPUT=[list,tuple]\n",
    "\n",
    "class CooccurrenceWorker(BaseCooccurrence):\n",
    "    def __init__(self):\n",
    "        name = 'cooccurrence'\n",
    "        self.inst = Cooccurrence(ngram_range=(2, 2), stop_words='english')\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "\n",
    "        # bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), vocabulary={'awesome unicorns': 0, 'batman forever': 1})\n",
    "        co_occurrences = self.inst.fit_transform(args[0])\n",
    "        # print('Printing sparse matrix:', co_occurrences)\n",
    "        # print(co_occurrences.todense())\n",
    "        sum_occ = np.sum(co_occurrences.todense(), axis=0)\n",
    "        # print('Sum of word-word occurrences:', sum_occ)\n",
    "\n",
    "        # Converting itertor to set\n",
    "        result = zip(self.inst.get_feature_names(), np.array(sum_occ)[0].tolist())\n",
    "        result_set = list(result)\n",
    "        return result_set, self.inst.vocab()\n",
    "\n",
    "class CooccurrenceManager:\n",
    "    def computeCooccurence(self, list):\n",
    "        com = defaultdict(lambda: defaultdict(int))\n",
    "        count_all = Counter()\n",
    "        count_all1 = Counter()\n",
    "\n",
    "        uniqueList = []\n",
    "        for _array in list:\n",
    "            for line in _array:\n",
    "                for word in line:\n",
    "                    if word not in uniqueList:\n",
    "                        uniqueList.append(word)\n",
    "\n",
    "                terms_bigram = bigrams(line)\n",
    "                # Update the counter\n",
    "                count_all.update(line)\n",
    "                count_all1.update(terms_bigram)\n",
    "\n",
    "                # Build co-occurrence matrix\n",
    "                for i in range(len(line) - 1):\n",
    "                    for j in range(i + 1, len(line)):\n",
    "                        w1, w2 = sorted([line[i], line[j]])\n",
    "                        if w1 != w2:\n",
    "                            com[w1][w2] += 1\n",
    "\n",
    "        com_max = []\n",
    "        # For each term, look for the most common co-occurrent terms\n",
    "        for t1 in com:\n",
    "            t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "            for t2, t2_count in t1_max_terms:\n",
    "                com_max.append(((t1, t2), t2_count))\n",
    "        # Get the most frequent co-occurrences\n",
    "        terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        return terms_max, uniqueList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co = CooccurrenceWorker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for sublist in mecab_nouns:\n",
    "    document = \",\".join(sublist)\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import itertools\n",
    "#merged = list(itertools.chain(*mecab_nouns))\n",
    "co_result, vocab = co.__call__(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.font_manager as fm\n",
    "import platform\n",
    "from matplotlib.ft2font import FT2Font\n",
    "import matplotlib as mpl\n",
    "\n",
    "class GraphMLCreator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.G = nx.Graph()\n",
    "\n",
    "        # Hack: offset the most central node to avoid too much overlap\n",
    "        self.rad0 = 0.3\n",
    "\n",
    "    def createGraphML(self, co_occurrence, word_hist, vocabulary, file):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        for obj in vocabulary:\n",
    "            G.add_node(obj)\n",
    "        # convert list to a single dictionary\n",
    "\n",
    "        for pair in co_occurrence:\n",
    "            node1 = ''\n",
    "            node2 = ''\n",
    "            for inner_pair in pair:\n",
    "\n",
    "                if type(inner_pair) is tuple:\n",
    "                    node1 = inner_pair[0]\n",
    "                    node2 = inner_pair[1]\n",
    "                elif type(inner_pair) is str:\n",
    "                    inner_pair=inner_pair.split()\n",
    "                    node1 = inner_pair[0]\n",
    "                    node2 = inner_pair[1]\n",
    "                elif type(inner_pair) is int:\n",
    "                    #print (\"X \" + node1 + \" == \" + node2 + \" == \" + str(inner_pair) + \" : \" + str(tuple[node1]))\n",
    "                    G.add_edge(node1, node2, weight=float(inner_pair))\n",
    "                elif type(inner_pair) is float:\n",
    "                    #print (\"X \" + node1 + \" == \" + node2 + \" == \" + str(inner_pair) + \" : \")\n",
    "                    G.add_edge(node1, node2, weight=float(inner_pair))\n",
    "        for word in word_hist:\n",
    "            G.add_node(word, count=word_hist[word])\n",
    "        self.G = G\n",
    "        print(self.G.number_of_nodes())\n",
    "        nx.write_graphml(G, file)\n",
    "\n",
    "    def createGraphMLWithThreshold(self, co_occurrence, word_hist, vocab, file, threshold=10.0):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        filtered_word_list=[]\n",
    "        for pair in co_occurrence:\n",
    "            node1 = ''\n",
    "            node2 = ''\n",
    "            for inner_pair in pair:\n",
    "                if type(inner_pair) is tuple:\n",
    "                    node1 = inner_pair[0]\n",
    "                    node2 = inner_pair[1]\n",
    "                elif type(inner_pair) is str:\n",
    "                    inner_pair=inner_pair.split()\n",
    "                    node1 = inner_pair[0]\n",
    "                    node2 = inner_pair[1]\n",
    "                elif type(inner_pair) is int:\n",
    "                    if float(inner_pair) >= threshold:\n",
    "                        #print (\"X \" + node1 + \" == \" + node2 + \" == \" + str(inner_pair) + \" : \" + str(tuple[node1]))\n",
    "                        G.add_edge(node1, node2, weight=float(inner_pair))\n",
    "                        if node1 not in filtered_word_list:\n",
    "                            filtered_word_list.append(node1)\n",
    "                        if node2 not in filtered_word_list:\n",
    "                            filtered_word_list.append(node2)\n",
    "                elif type(inner_pair) is float:\n",
    "                    if float(inner_pair) >= threshold:\n",
    "                        #print (\"X \" + node1 + \" == \" + node2 + \" == \" + str(inner_pair) + \" : \")\n",
    "                        G.add_edge(node1, node2, weight=float(inner_pair))\n",
    "                        if node1 not in filtered_word_list:\n",
    "                            filtered_word_list.append(node1)\n",
    "                        if node2 not in filtered_word_list:\n",
    "                            filtered_word_list.append(node2)\n",
    "        for word in word_hist:\n",
    "            if word in filtered_word_list:\n",
    "                G.add_node(word, count=word_hist[word])\n",
    "\n",
    "        self.G = G\n",
    "        print(self.G.number_of_nodes())\n",
    "        nx.write_graphml(G, file)\n",
    "\n",
    "    def centrality_layout(self):\n",
    "        centrality = nx.eigenvector_centrality_numpy(self.G)\n",
    "        \"\"\"Compute a layout based on centrality.\n",
    "        \"\"\"\n",
    "        # Create a list of centralities, sorted by centrality value\n",
    "        cent = sorted(centrality.items(), key=lambda x:float(x[1]), reverse=True)\n",
    "        nodes = [c[0] for c in cent]\n",
    "        cent  = np.array([float(c[1]) for c in cent])\n",
    "        rad = (cent - cent[0])/(cent[-1]-cent[0])\n",
    "        rad = self.rescale_arr(rad, self.rad0, 1)\n",
    "        angles = np.linspace(0, 2*np.pi, len(centrality))\n",
    "        layout = {}\n",
    "        for n, node in enumerate(nodes):\n",
    "            r = rad[n]\n",
    "            th = angles[n]\n",
    "            layout[node] = r*np.cos(th), r*np.sin(th)\n",
    "        return layout\n",
    "\n",
    "    def plot_graph(self, title=None, file='graph.png'):\n",
    "        from matplotlib.font_manager import _rebuild\n",
    "        _rebuild()\n",
    "\n",
    "        font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'\n",
    "\n",
    "        font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "        plt.rc('font', family=font_name)\n",
    "        plt.rc('axes', unicode_minus=False)\n",
    "        # 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "        mpl.rcParams['axes.unicode_minus'] = False\n",
    "        #print('버전: ', mpl.__version__)\n",
    "        #print('설치 위치: ', mpl.__file__)\n",
    "        #print('설정 위치: ', mpl.get_configdir())\n",
    "        #print('캐시 위치: ', mpl.get_cachedir())\n",
    "\n",
    "        # size, family\n",
    "        print('# 설정 되어있는 폰트 사이즈')\n",
    "        print(plt.rcParams['font.size'])\n",
    "        print('# 설정 되어있는 폰트 글꼴')\n",
    "        print(plt.rcParams['font.family'])\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        pos = self.centrality_layout()\n",
    "\n",
    "        \"\"\"Conveniently summarize graph visually\"\"\"\n",
    "        # config parameters\n",
    "        edge_min_width= 3\n",
    "        edge_max_width= 12\n",
    "        label_font = 18\n",
    "        node_font = 22\n",
    "        node_alpha = 0.4\n",
    "        edge_alpha = 0.55\n",
    "        edge_cmap = plt.cm.Spectral\n",
    "\n",
    "        # Create figure\n",
    "        if fig is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        else:\n",
    "            ax = fig.add_subplot(111)\n",
    "        fig.subplots_adjust(0,0,1)\n",
    "\n",
    "        font = FT2Font(font_path)\n",
    "\n",
    "        # Plot nodes with size according to count\n",
    "        sizes = []\n",
    "        degrees = []\n",
    "        for n, d in self.G.nodes(data=True):\n",
    "            sizes.append(d['count'])\n",
    "            degrees.append(self.G.degree(n))\n",
    "\n",
    "        sizes = self.rescale_arr(np.array(sizes, dtype=float), 100, 1000)\n",
    "\n",
    "        # Compute layout and label edges according to weight\n",
    "        pos = nx.spectral_layout(self.G) if pos is None else pos\n",
    "        labels = {}\n",
    "        width = []\n",
    "        for n1, n2, d in self.G.edges(data=True):\n",
    "            w = d['weight']\n",
    "            labels[n1, n2] = w\n",
    "            width.append(w)\n",
    "\n",
    "        width = self.rescale_arr(np.array(width, dtype=float), edge_min_width,\n",
    "                            edge_max_width)\n",
    "\n",
    "        # Draw\n",
    "        nx.draw_networkx_nodes(self.G, pos, node_size=sizes, node_color=degrees,\n",
    "                               alpha=node_alpha)\n",
    "        nx.draw_networkx_edges(self.G, pos, width=width, edge_color=width,\n",
    "                               edge_cmap=edge_cmap, alpha=edge_alpha)\n",
    "        #nx.draw_networkx_edge_labels(self.G, pos, edge_labels=labels,\n",
    "                                     #font_size=label_font)\n",
    "        nx.draw_networkx_labels(self.G, pos, font_size=node_font, font_family=font_name, font_weight='bold')\n",
    "\n",
    "        if title is not None:\n",
    "            ax.set_title(title, fontsize=label_font)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Mark centrality axes\n",
    "        kw = dict(color='k', linestyle='-')\n",
    "        cross = [ax.axhline(0, **kw), ax.axvline(self.rad0, **kw)]\n",
    "        [l.set_zorder(0) for l in cross]\n",
    "        \n",
    "        plt.savefig(file)\n",
    "        plt.show()\n",
    "\n",
    "    def rescale_arr(self, arr, amin, amax):\n",
    "        \"\"\"Rescale an array to a new range.\n",
    "        Return a new array whose range of values is (amin, amax).\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : array-like\n",
    "        amin : float\n",
    "          new minimum value\n",
    "        amax : float\n",
    "          new maximum value\n",
    "        Examples\n",
    "        --------\n",
    "        >>> a = np.arange(5)\n",
    "        >>> rescale_arr(a,3,6)\n",
    "        array([ 3.  ,  3.75,  4.5 ,  5.25,  6.  ])\n",
    "        \"\"\"\n",
    "\n",
    "        # old bounds\n",
    "        m = arr.min()\n",
    "        M = arr.max()\n",
    "        # scale/offset\n",
    "        s = float(amax - amin) / (M - m)\n",
    "        d = amin - s * m\n",
    "\n",
    "        # Apply clip before returning to cut off possible overflows outside the\n",
    "        # intended range due to roundoff error, so that we can absolutely guarantee\n",
    "        # that on output, there are no values > amax or < amin.\n",
    "        return np.clip(s * arr + d, amin, amax)\n",
    "\n",
    "    def summarize_centrality(self, limit=10):\n",
    "        centrality = nx.eigenvector_centrality_numpy(self.G)\n",
    "        c = centrality.items()\n",
    "        c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "        print('\\nGraph centrality')\n",
    "        count=0\n",
    "        for node, cent in c:\n",
    "            if count>limit:\n",
    "                break\n",
    "            print (\"%15s: %.3g\" % (node, float(cent)))\n",
    "            count+=1\n",
    "\n",
    "    def sort_freqs(self, freqs):\n",
    "        \"\"\"Sort a word frequency histogram represented as a dictionary.\n",
    "        Parameters\n",
    "        ----------\n",
    "        freqs : dict\n",
    "          A dict with string keys and integer values.\n",
    "        Return\n",
    "        ------\n",
    "        items : list\n",
    "          A list of (count, word) pairs.\n",
    "        \"\"\"\n",
    "        items = freqs.items()\n",
    "        items.sort(key=lambda wc: wc[1])\n",
    "        return items\n",
    "\n",
    "    def plot_word_histogram(self, freqs, show=10, title=None):\n",
    "        \"\"\"Plot a histogram of word frequencies, limited to the top `show` ones.\n",
    "        \"\"\"\n",
    "        sorted_f = self.sort_freqs(freqs) if isinstance(freqs, dict) else freqs\n",
    "\n",
    "        # Don't show the tail\n",
    "        if isinstance(show, int):\n",
    "            # interpret as number of words to show in histogram\n",
    "            show_f = sorted_f[-show:]\n",
    "        else:\n",
    "            # interpret as a fraction\n",
    "            start = -int(round(show * len(freqs)))\n",
    "            show_f = sorted_f[start:]\n",
    "\n",
    "        # Now, extract words and counts, plot\n",
    "        n_words = len(show_f)\n",
    "        ind = np.arange(n_words)\n",
    "        words = [i[0] for i in show_f]\n",
    "        counts = [i[1] for i in show_f]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        if n_words <= 20:\n",
    "            # Only show bars and x labels for small histograms, they don't make\n",
    "            # sense otherwise\n",
    "            ax.bar(ind, counts)\n",
    "            ax.set_xticks(ind)\n",
    "            ax.set_xticklabels(words, rotation=45)\n",
    "            fig.subplots_adjust(bottom=0.25)\n",
    "        else:\n",
    "            # For larger ones, do a step plot\n",
    "            ax.step(ind, counts)\n",
    "\n",
    "        # If it spans more than two decades, use a log scale\n",
    "        if float(max(counts)) / min(counts) > 100:\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(documents)\n",
    "word_list = cv.get_feature_names();\n",
    "count_list = cv_fit.toarray().sum(axis=0)\n",
    "word_hist = dict(zip(word_list, count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1877\n"
     ]
    }
   ],
   "source": [
    "graph_builder = GraphMLCreator()\n",
    "graph_builder.createGraphMLWithThreshold(co_result, word_hist, vocab, file=\"/Users/SeoyeonHong/Desktop/text_mining/qna_topic_preventive.graphml\", threshold=30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
