{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Topic Modeling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_df = pd.read_csv('/Users/SeoyeonHong/Desktop/text_mining/topic_modeling/naver_kin/naver_q1_topic_representation.tsv', sep = '\\t')\n",
    "aha_df = pd.read_csv('/Users/SeoyeonHong/Desktop/text_mining/topic_modeling/aha/aha_qna_topic_representation.tsv', sep = '\\t')\n",
    "hidoc_df = pd.read_csv('/Users/SeoyeonHong/Desktop/text_mining/topic_modeling/hidoc/hidoc_qna_topic_representation.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DF (Naver Kin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_qna_df = pd.read_csv(\"naver_df.csv\")\n",
    "naver_qna_df.rename(columns={'questions':'Text'}, inplace=True)\n",
    "naver_qna_df = naver_qna_df.drop([\"Unnamed: 0\"], axis=1)\n",
    "naver_qna_df = naver_qna_df.drop([\"Text\"], axis=1)\n",
    "naver_qna_df['Text'] = naver_qna_df[\"q_title\"].str.cat(naver_qna_df[\"q_content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_qna_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_answers1 = pd.read_csv(\"/Users/SeoyeonHong/Desktop/text_mining/Data/naver_expert_qna_df_20200224_20200228.tsv\", sep = '\\t')\n",
    "naver_answers2 = pd.read_csv(\"/Users/SeoyeonHong/Desktop/text_mining/Data/naver_expert_qna_df_20200229_20200307.tsv\", sep = '\\t')\n",
    "naver_answers3 = pd.read_csv(\"/Users/SeoyeonHong/Desktop/text_mining/Data/naver_expert_qna_df_20200308_20200318.tsv\", sep = '\\t')\n",
    "naver_answers4 = pd.read_csv(\"/Users/SeoyeonHong/Desktop/text_mining/Data/naver_expert_qna_df_20200319_20200328.tsv\", sep = '\\t')\n",
    "naver_answers5 = pd.read_csv(\"/Users/SeoyeonHong/Desktop/text_mining/Data/naver_expert_qna_df_20200329_20200406.tsv\", sep = '\\t')\n",
    "naver_answers6 = pd.read_csv(\"/Users/SeoyeonHong/Desktop/text_mining/Data/naver_expert_qna_df_20200407_20200417.tsv\", sep = '\\t')\n",
    "naver_answers7 = pd.read_csv(\"/Users/SeoyeonHong/Desktop/text_mining/Data/naver_expert_qna_df_20200418_20200425.tsv\", sep = '\\t')\n",
    "naver_answers8 = pd.read_csv(\"/Users/SeoyeonHong/Desktop/text_mining/Data/naver_expert_qna_df_20191201_20200223.tsv\", sep = '\\t')\n",
    "naver_answers = pd.concat([naver_answers1, naver_answers2, naver_answers3, naver_answers4, naver_answers5, naver_answers6, naver_answers7, naver_answers8])\n",
    "naver_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_naver = naver_df.loc[naver_df['Topic_Num'] == 31] #클러스터에 해당하는 index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_cluster = pd.merge(cluster_naver, naver_qna_df, how='left', left_on = 'Text', right_on = 'Text')\n",
    "naver_cluster = naver_cluster.drop([\"Text\"], axis=1)\n",
    "naver_cluster = naver_cluster.drop([\"Unnamed: 0\"], axis=1)\n",
    "naver_cluster_w_ans = pd.merge(naver_cluster, naver_answers, how='left', left_on = 'q_title', right_on = 'titles')\n",
    "naver_cluster_w_ans = naver_cluster_w_ans.dropna()\n",
    "naver_cluster_w_ans = naver_cluster_w_ans.sort_values(by='Topic_Perc_Contrib', ascending=False)\n",
    "naver_cluster_w_ans = naver_cluster_w_ans.drop([\"Unnamed: 0\"], axis=1)\n",
    "naver_cluster_w_ans = naver_cluster_w_ans.drop([\"qna_urls\"], axis=1)\n",
    "naver_cluster_w_ans = naver_cluster_w_ans.drop([\"q_title\"], axis=1)\n",
    "naver_cluster_w_ans = naver_cluster_w_ans.drop([\"q_content\"], axis=1)\n",
    "naver_cluster_w_ans = naver_cluster_w_ans.drop([\"dates\"], axis=1)\n",
    "naver_cluster_w_ans.rename({'date':'dates'}, axis=1, inplace = True)\n",
    "source = [\"naver\"]* naver_cluster_w_ans.shape[0]\n",
    "naver_cluster_w_ans[\"source\"] = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "naver_cluster_w_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DF (Aha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aha_qna_df = pd.read_excel('/Users/SeoyeonHong/Desktop/text_mining/Data/aha_qna_df.xlsx')\n",
    "aha_qna_df = aha_qna_df.fillna(\" \")\n",
    "aha_qna_df[\"n_questions\"] = aha_qna_df[\"titles\"].str.cat(aha_qna_df[\"questions\"], sep = \" \")\n",
    "aha_qna_df = aha_qna_df.drop_duplicates()\n",
    "\n",
    "for line in aha_qna_df['dates']:\n",
    "    substring = str(line).replace(\"\\n\",\"\")\n",
    "    aha_qna_df['dates'] = aha_qna_df['dates'].replace(line, substring)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_aha = aha_df.loc[aha_df['Topic_Num'] == 15] #클러스터에 해당하는 index number\n",
    "cluster_aha = cluster_aha.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aha_cluster = pd.merge(cluster_aha, aha_qna_df, how='left', left_on = 'Text', right_on = 'questions')\n",
    "aha_cluster = aha_cluster.drop([\"Text\"], axis=1)\n",
    "aha_cluster = aha_cluster.drop([\"qna_urls\"], axis=1)\n",
    "aha_cluster = aha_cluster.drop([\"n_questions\"], axis=1)\n",
    "aha_cluster = aha_cluster.sort_values(by='Topic_Perc_Contrib', ascending=False)\n",
    "source = [\"aha\"]* aha_cluster.shape[0]\n",
    "aha_cluster[\"source\"] = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aha_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DF (HiDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidoc_qna_df = pd.read_csv('/Users/SeoyeonHong/Desktop/text_mining/Data/hidoc_qna_df.tsv', sep = '\\t')\n",
    "hidoc_qna_df = hidoc_qna_df.fillna(\" \")\n",
    "hidoc_qna_df = hidoc_qna_df.drop([\"Unnamed: 0\"], axis = 1)\n",
    "hidoc_qna_df[\"n_questions\"] = hidoc_qna_df[\"titles\"].str.cat(hidoc_qna_df[\"questions\"], sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_hidoc_1 = hidoc_df.loc[hidoc_df['Topic_Num'] == 19] # 클러스터 index number에 따라\n",
    "cluster_hidoc_2 = hidoc_df.loc[hidoc_df['Topic_Num'] == 22]\n",
    "cluster_hidoc_3 = hidoc_df.loc[hidoc_df['Topic_Num'] == 29]\n",
    "cluster_hidoc = pd.concat([cluster_hidoc_1, cluster_hidoc_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidoc_cluster = pd.merge(cluster_hidoc, hidoc_qna_df, how='left', left_on = 'Text', right_on = 'questions')\n",
    "hidoc_cluster = hidoc_cluster.drop([\"Unnamed: 0\"], axis = 1)\n",
    "hidoc_cluster = hidoc_cluster.drop([\"prof_agree\"], axis = 1)\n",
    "hidoc_cluster = hidoc_cluster.drop([\"user_agree\"], axis = 1)\n",
    "hidoc_cluster = hidoc_cluster.drop([\"poser_thank\"], axis = 1)\n",
    "hidoc_cluster = hidoc_cluster.drop([\"qna_urls\"], axis = 1)\n",
    "hidoc_cluster = hidoc_cluster.drop([\"Text\"], axis = 1)\n",
    "hidoc_cluster = hidoc_cluster.drop([\"n_questions\"], axis = 1)\n",
    "hidoc_cluster = hidoc_cluster.sort_values(by='Topic_Perc_Contrib', ascending=False)\n",
    "source = [\"hidoc\"]* hidoc_cluster.shape[0]\n",
    "hidoc_cluster[\"source\"] = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidoc_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_representation_cluster = pd.concat([naver_cluster_w_ans, aha_cluster, hidoc_cluster])\n",
    "top_representation_cluster = top_representation_cluster.sort_values(by='Topic_Perc_Contrib', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Analysis 1 (Answer Length / Key Sentence Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key sentence extraction (reference: https://lovit.github.io/nlp/2019/04/30/textrank/)\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.3):\n",
    "    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "\n",
    "    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
    "\n",
    "def textrank_sent_sim(s1, s2):\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base\n",
    "\n",
    "def cosine_sent_sim(s1, s2):\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)\n",
    "    return prod / (norm1 * norm2)\n",
    "\n",
    "def textrank_keysentence(sents, tokenize, min_count, similarity, df=0.85, max_iter=30, topk=5):\n",
    "    g = sent_graph(sents, tokenize, min_count, min_sim, similarity)\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    keysents = [(idx, R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "    return keysents\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "def komoran_tokenize(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "from textrank import KeysentenceSummarizer\n",
    "df = top_representation_cluster1.loc[top_representation_cluster1['Topic_Perc_Contrib'] >= 0.2] # 0.4\n",
    "df = df.drop_duplicates(['answers'])\n",
    "\n",
    "answers = []\n",
    "ans_length = [] # answer length\n",
    "key_sent = [] # key sentence\n",
    "key_sent_index = [] # key sentence location\n",
    "total_sents = [] # number of sentences within an answer\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    para = df.iloc[i].answers.strip(\"['']\")\n",
    "    para = para.replace(\"\\\\xa0\", \" \").replace(\"\\\\u200b\", \" \").replace(\"\\\\\", \" \")\n",
    "    answers.append(para)\n",
    "    # answer length\n",
    "    text_length = len(para)\n",
    "    ans_length.append(text_length)\n",
    "    sents = kss.split_sentences(para)\n",
    "    total_sentences = len(sents) # number of sents\n",
    "    total_sents.append(total_sentences)\n",
    "    # key sentence extraction\n",
    "    summarizer = KeysentenceSummarizer(tokenize = komoran_tokenize, min_sim = 0.5)\n",
    "    try:\n",
    "        keysents = summarizer.summarize(sents, topk=10)\n",
    "        key_sent.append(keysents[0][2]) # key sent\n",
    "        key_sent_index.append(keysents[0][0]) # key sent location\n",
    "    except ValueError:\n",
    "        key_sent.append(\"None\")\n",
    "        key_sent_index.append(\"None\")\n",
    "\n",
    "data = zip(answers, ans_length, key_sent, key_sent_index, total_sents)\n",
    "keysent_df = pd.DataFrame(data, columns = ['answers', 'ans_length', 'key_sent', 'key_sent_index', 'total_sents'])\n",
    "keysent_df = keysent_df[keysent_df.total_sents != 1]\n",
    "keysent_df = keysent_df[keysent_df.key_sent_index != \"None\"]\n",
    "keysent_df.to_csv(\"/Users/SeoyeonHong/Desktop/text_mining/cluster1_keysentence.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Analysis 2 (Average Nouns/Verbs Occurrences in Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "def count_nouns(substring):\n",
    "    sent_pos = kkma.pos(substring)\n",
    "    nouns = [n for n, tag in sent_pos if tag in [\"NNG\",\"NNP\"]]\n",
    "    #print(nouns)\n",
    "    global noun_count\n",
    "    noun_count = len(nouns)\n",
    "\n",
    "def count_verbs(substring):\n",
    "    sent_pos = kkma.pos(substring)\n",
    "    verbs = [v for v, tag in sent_pos if tag in [\"VV\", \"VX\", \"VA\", \"VCP\"]]\n",
    "    #print(verbs)\n",
    "    global verb_count\n",
    "    verb_count = len(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = top_representation_cluster.loc[top_representation_cluster['Topic_Perc_Contrib'] >= 0.2]\n",
    "df = df.drop_duplicates(['answers'])\n",
    "\n",
    "import kss\n",
    "\n",
    "answers = []\n",
    "average_noun_counts = []\n",
    "average_verb_counts = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    # text pre-processing\n",
    "    para = df.iloc[i].answers.strip(\"['']\")\n",
    "    para = para.replace(\"\\\\xa0\", \" \").replace(\"\\\\u200b\", \" \").replace(\"\\\\\", \" \")\n",
    "    answers.append(para)\n",
    "    # splitting sentences\n",
    "    sents = kss.split_sentences(para)\n",
    "    # counting nouns and verbs in each sentence\n",
    "    noun_count_per_sentence = []\n",
    "    verb_count_per_sentence = []\n",
    "    for sent in sents[:]:\n",
    "        count_nouns(sent)\n",
    "        noun_count_per_sentence.append(noun_count)\n",
    "        count_verbs(sent)\n",
    "        verb_count_per_sentence.append(verb_count)\n",
    "    # calculating average\n",
    "    try:\n",
    "        average_noun_counts.append(sum(noun_count_per_sentence) / len(sents))\n",
    "        average_verb_counts.append(sum(verb_count_per_sentence) / len(sents))\n",
    "    except ZeroDivisionError:\n",
    "        average_noun_counts.append(\"None\")\n",
    "        average_verb_counts.append(\"None\")\n",
    "\n",
    "\n",
    "data = zip(answers, average_noun_counts, average_verb_counts)\n",
    "pos_count = pd.DataFrame(data, columns = ['answers', 'average_noun_counts', 'average_verb_counts'])\n",
    "pos_count = pos_count[pos_count.average_noun_counts != \"None\"]\n",
    "pos_count\n",
    "pos_count.to_csv(\"/Users/SeoyeonHong/Desktop/text_mining/cluster1_pos_count.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Analysis 3 (Calculating Uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = top_representation_cluster1.loc[top_representation_cluster1['Topic_Perc_Contrib'] >= 0.2] # 0.4\n",
    "df = df.drop_duplicates(['answers'])\n",
    "\n",
    "import kss\n",
    "from textrank import KeysentenceSummarizer\n",
    "ambiguous_phrases = [\"생각됩니다\", \"라고 합니다\", \"다고 합니다\", \"라고 말할\", \"라고 말하\", \"라고 할 수\", \"것 같\", \"생각합니다\", \\\n",
    "                     \"사료됩니다\", \"수도\", \"아닐까 싶습니다\", \"을 겁니다\", \"일 겁니다\", \"수 있습니다\", \"수 있어요\", \"듯 합니다\"]\n",
    "\n",
    "answers = [] # answer\n",
    "total_sents_num = [] # total number of sentences\n",
    "ambiguous_sents_num = [] # number of ambiguous sentences\n",
    "answer_ambiguity = [] # answer ambiguity\n",
    "key_sent_ambiguity = [] # whether key sentence is ambiguous or not\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    para = df.iloc[i].answers.strip(\"['']\")\n",
    "    para = para.replace(\"\\\\xa0\", \" \").replace(\"\\\\u200b\", \" \").replace(\"\\\\\", \" \")\n",
    "    answers.append(para)\n",
    "    kss_sents = kss.split_sentences(para)\n",
    "    # calculating answer ambiguity\n",
    "    total_sentences_number = len(kss_sents) \n",
    "    total_sents_num.append(total_sentences_number) # total number of sentences\n",
    "    ambiguous_sentences = []\n",
    "    for sent in kss_sents:\n",
    "        if any(phrase in sent for phrase in ambiguous_phrases):\n",
    "            ambiguous_sentences.append(sent)\n",
    "    ambiguous_sents_num.append(len(ambiguous_sentences)) # number of ambiguous sentences\n",
    "    try:\n",
    "        answer_ambiguity.append(round(len(ambiguous_sentences)/total_sentences_number, 3))\n",
    "    except ZeroDivisionError: # if no answer\n",
    "        answer_ambiguity.append(\"None\")\n",
    "    # whether key sentence is ambiguous or not\n",
    "    summarizer = KeysentenceSummarizer(tokenize = komoran_tokenize, min_sim = 0.5)\n",
    "    try:\n",
    "        keysents = summarizer.summarize(kss_sents, topk=10)\n",
    "        key_sentence = keysents[0][2]\n",
    "    except ValueError: # if no answer\n",
    "        key_sentence = \"\"\n",
    "    if any(phrase in key_sentence for phrase in ambiguous_phrases):\n",
    "        key_sent_ambiguity.append(1)\n",
    "    else:\n",
    "        key_sent_ambiguity.append(0)\n",
    "\n",
    "data = zip(answers, total_sents_num, ambiguous_sents_num, answer_ambiguity, key_sent_ambiguity)\n",
    "ambiguity_df = pd.DataFrame(data, columns = ['answer','total_sents_num', 'ambiguous_sents_num', 'answer_ambiguity', 'key_sent_ambiguity'])\n",
    "ambiguity_df = ambiguity_df[ambiguity_df.answer_ambiguity != \"None\"]\n",
    "ambiguity_df.to_csv(\"/Users/SeoyeonHong/Desktop/text_mining/cluster1_ambiguity.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
